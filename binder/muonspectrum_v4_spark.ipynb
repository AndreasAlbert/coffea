{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Four Muon Spectrum - with apache spark\n",
    "\n",
    "This code is another showcase of the awkward array toolset, and utilizing FCAT histograms in addition to advanced functionality.\n",
    "This shows the analysis object syntax implemented by FCAT `JaggedCandidateArray`, along with a multi-tiered physics selection, and the usage of an accumulator class provided by FCAT.  We now add in the concept of corrections as well in the case of a Monte-Carlo sample.\n",
    "\n",
    "Instead of using a local multithreading option for scale-out, we use apache spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pyspark.sql\n",
    "\n",
    "from fnal_column_analysis_tools.processor.spark.detail import (_spark_initialize,\n",
    "                                                               _spark_stop)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring Spark\n",
    "The spark session is configured by specifying a 'master', the computer that submits the jobs.\n",
    "In this case the master is 'local[\\*]' which requests that spark use all available cores on your computer for computation. The 'execution.arrow' related statements drive optimizations within Apache Spark that allow columnar data processing to be done efficiently as vectorized operations.\n",
    "\n",
    "At the ACCRE cluster if master is not specified you will instead launch jobs that can use all the cores available in in the system (up to nearly 1000). However, for the demonstrations today we just want to show how to use spark to do the basics of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "# This function creates the spark session based on the configuration passed to it.\n",
    "spark = _spark_initialize(config=spark_config, log_level='ERROR', spark_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from fnal_column_analysis_tools import hist\n",
    "from fnal_column_analysis_tools.hist import plot\n",
    "from fnal_column_analysis_tools.analysis_objects import JaggedCandidateArray\n",
    "import fnal_column_analysis_tools.processor as processor\n",
    "from awkward import JaggedArray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC to see the expected methods and what they are supposed to do\n",
    "class FancyDimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, columns = []):\n",
    "        self._columns = columns\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"Primary dataset\")\n",
    "        mass_axis = hist.Bin(\"mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 600, 0.25, 300)\n",
    "        pt_axis = hist.Bin(\"pt\", r\"$p_{T,\\mu}$ [GeV]\", 3000, 0.25, 300)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator({\n",
    "            'mass': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'mass_near': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'mass_far': hist.Hist(\"Counts\", dataset_axis, mass_axis),\n",
    "            'pt_lead': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'pt_trail': hist.Hist(\"Counts\", dataset_axis, pt_axis),\n",
    "            'cutflow': processor.defaultdict_accumulator(int),\n",
    "        })\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        dataset = df['dataset']\n",
    "        muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "            df['nMuon'],\n",
    "            pt=df['Muon_pt'],\n",
    "            eta=df['Muon_eta'],\n",
    "            phi=df['Muon_phi'],\n",
    "            mass=df['Muon_mass'],\n",
    "            charge=df['Muon_charge'],\n",
    "            softId=df['Muon_softId'],\n",
    "            tightId=df['Muon_tightId']\n",
    "            )        \n",
    "        \n",
    "        output['cutflow']['all events'] += muons.size\n",
    "        \n",
    "        soft_id = (muons.softId > 0)\n",
    "        muons = muons[soft_id]\n",
    "        output['cutflow']['soft id'] += soft_id.any().sum()\n",
    "        \n",
    "        twomuons = (muons.counts >= 2)\n",
    "        output['cutflow']['two muons'] += twomuons.sum()\n",
    "        \n",
    "        dimuons = muons[twomuons].distincts()\n",
    "        \n",
    "        twodimuons = (dimuons.counts >= 2)\n",
    "        output['cutflow']['>= two dimuons'] += twodimuons.sum()\n",
    "        dimuons = dimuons[twodimuons]\n",
    "        \n",
    "        opposite_charge = (dimuons.i0['charge'] * dimuons.i1['charge'] == -1)\n",
    "        \n",
    "        dimuons = dimuons[opposite_charge]\n",
    "        output['cutflow']['opposite charge'] += opposite_charge.any().sum()\n",
    "        \n",
    "        mass_20GeV = (dimuons.mass > 35)\n",
    "        dimuons = dimuons[mass_20GeV]\n",
    "        \n",
    "        exactlytwodimuons = (dimuons.counts == 2)\n",
    "        output['cutflow']['== two dimuons'] += exactlytwodimuons.sum()\n",
    "        dimuons = dimuons[exactlytwodimuons].compact()\n",
    "        \n",
    "        leading_mu = (dimuons.i0.pt.content > dimuons.i1.pt.content)\n",
    "        pt_lead = JaggedArray.fromoffsets(dimuons.offsets, np.where(leading_mu, \n",
    "                                                                    dimuons.i0.pt.content, dimuons.i1.pt.content))\n",
    "        pt_trail = JaggedArray.fromoffsets(dimuons.offsets, np.where(~leading_mu, \n",
    "                                                                     dimuons.i0.pt.content, dimuons.i1.pt.content))\n",
    "        \n",
    "        near_z = np.abs(dimuons.mass - 91.118).argmin()\n",
    "        far_z = np.abs(dimuons.mass - 91.118).argmax()\n",
    "        \n",
    "        output['mass'].fill(dataset=dataset,\n",
    "                            mass=dimuons.p4.sum().mass)\n",
    "        output['mass_near'].fill(dataset=dataset, \n",
    "                                 mass=dimuons.mass[near_z].flatten())\n",
    "        output['mass_far'].fill(dataset=dataset, \n",
    "                                mass=dimuons.mass[far_z].flatten())\n",
    "        output['pt_lead'].fill(dataset=dataset,\n",
    "                               pt=pt_lead.flatten())\n",
    "        output['pt_trail'].fill(dataset=dataset,\n",
    "                                pt=pt_trail.flatten())\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fnal_column_analysis_tools.processor import run_spark_job\n",
    "from fnal_column_analysis_tools.processor.spark.spark_executor import spark_executor\n",
    "\n",
    "tstart = time.time()    \n",
    "\n",
    "fileset = {\n",
    "    'DoubleMuon': [\n",
    "        'data/Run2012B_DoubleMuParked.root',\n",
    "        'data/Run2012C_DoubleMuParked.root',\n",
    "    ],\n",
    "    'ZZ to 4mu': [\n",
    "        'data/ZZTo4mu.root'\n",
    "    ]\n",
    "}\n",
    "\n",
    "columns = ['nMuon','Muon_pt','Muon_eta','Muon_phi','Muon_mass',\n",
    "           'Muon_charge','Muon_softId','Muon_tightId']\n",
    "proc = FancyDimuonProcessor(columns=columns)\n",
    "\n",
    "output = run_spark_job(filelist, processor_instance=proc, executor=spark_executor, spark=spark, thread_workers=2)\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_spark_stop(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = plot.plot1d(output['mass'], overlay='dataset')\n",
    "ax.set_xlim(70,150)\n",
    "ax.set_ylim(0, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = plot.plot1d(output['mass_near'], overlay='dataset')\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_xlim(60,120)\n",
    "ax.set_ylim(0.1, 7500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = plot.plot1d(output['mass_far'], overlay='dataset')\n",
    "#ax.set_xscale('log')\n",
    "#ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = plot.plot1d(output['pt_lead'], overlay='dataset')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 5e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax, _ = plot.plot1d(output['pt_trail'], overlay='dataset')\n",
    "#ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylim(0.1, 2e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
